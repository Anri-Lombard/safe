{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-15 12:38:48,928] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import uuid\n",
    "import safe\n",
    "import transformers\n",
    "import evaluate\n",
    "from dataclasses import dataclass, field\n",
    "from loguru import logger\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import set_seed\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers import TrainingArguments\n",
    "from safe.trainer.model import SAFEDoubleHeadsModel\n",
    "from safe.tokenizer import SAFETokenizer\n",
    "from safe.trainer.data_utils import get_dataset\n",
    "from safe.trainer.collator import SAFECollator\n",
    "from safe.trainer.trainer_utils import SAFETrainer\n",
    "from safe.trainer.cli import ModelArguments\n",
    "from safe.trainer.cli import DataArguments\n",
    "from safe.trainer.cli import train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"../data/\"\n",
    "tokenizer = \"../tokenizer/tokenizer-custom.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dt = get_dataset(dataset, streaming=True, tokenize_column=\"input\", property_column=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args_1 = ModelArguments(config=None, tokenizer=tokenizer, prop_loss_coeff=1e-2, num_labels=18, include_descriptors=False, wandb_watch=\"gradient\")\n",
    "model_args_2 = ModelArguments(config=None, tokenizer=tokenizer, prop_loss_coeff=1e-2, num_labels=18, include_descriptors=True, wandb_watch=\"gradient\")\n",
    "data_args = DataArguments(dataset=dataset,  streaming=True, text_column=\"input\", is_tokenized=False, property_column=None,  max_eval_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "ddp = False\n",
    "gradient_accumulation_steps = 2\n",
    "\n",
    "batch_size = 32\n",
    "warmup_steps = 10\n",
    "num_epochs = 1\n",
    "learning_rate = 1e-5\n",
    "\n",
    "logging_steps = 10\n",
    "output_dir = \"../output/\"\n",
    "max_steps = 50\n",
    "save_steps = 25\n",
    "cache_dir = None\n",
    "eval_accumulation_steps = 100 # uses this to avoid \n",
    "eval_delay=None\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        do_train=True, \n",
    "        do_eval=True, \n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        warmup_steps=warmup_steps,\n",
    "        num_train_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        logging_steps=logging_steps,\n",
    "        #eval_steps=0.5,\n",
    "        optim=\"adamw_torch\",\n",
    "        output_dir=output_dir,\n",
    "        report_to=\"wandb\",\n",
    "        save_safetensors=True,\n",
    "        torch_compile=True,\n",
    "        max_steps=max_steps,\n",
    "        log_level=\"warning\",\n",
    "        label_names=[\"labels\", \"mc_labels\"],\n",
    "        save_steps=save_steps,\n",
    "        eval_delay=eval_delay,\n",
    "        eval_accumulation_steps=eval_accumulation_steps\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf ../output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▃▅▆██</td></tr><tr><td>train/global_step</td><td>▁▃▅▆██</td></tr><tr><td>train/learning_rate</td><td>█▆▅▃▁</td></tr><tr><td>train/loss</td><td>█▅▆▃▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>50</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>93.6365</td></tr><tr><td>train/total_flos</td><td>515540443833600.0</td></tr><tr><td>train/train_loss</td><td>100.1764</td></tr><tr><td>train/train_runtime</td><td>30.5729</td></tr><tr><td>train/train_samples_per_second</td><td>209.335</td></tr><tr><td>train/train_steps_per_second</td><td>1.635</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">safe-model-d948823a</strong> at: <a href='https://wandb.ai/maclandrol/safe-gpt2/runs/lcltfq2e' target=\"_blank\">https://wandb.ai/maclandrol/safe-gpt2/runs/lcltfq2e</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230815_123900-lcltfq2e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|trainer.py:2596] 2023-08-15 12:41:01,118 >> There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
      "cat: /sys/module/amdgpu/initstate: No such file or directory\n",
      "ERROR:root:Driver not initialized (amdgpu not found in modules)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/emmanuel_invivoai_com/dev/safe/expts/notebook/wandb/run-20230815_124102-l72d5xvf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/maclandrol/safe-gpt2/runs/l72d5xvf' target=\"_blank\">safe-model-dd09cb7b</a></strong> to <a href='https://wandb.ai/maclandrol/safe-gpt2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/maclandrol/safe-gpt2' target=\"_blank\">https://wandb.ai/maclandrol/safe-gpt2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/maclandrol/safe-gpt2/runs/l72d5xvf' target=\"_blank\">https://wandb.ai/maclandrol/safe-gpt2/runs/l72d5xvf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emmanuel_invivoai_com/mambaforge/envs/safe-space/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:00, Epoch 1/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       1.02\n",
      "  total_flos               =   488961GF\n",
      "  train_loss               =     1.8506\n",
      "  train_runtime            = 0:00:07.80\n",
      "  train_samples_per_second =     819.47\n",
      "  train_steps_per_second   =      6.402\n",
      "***** eval metrics *****\n",
      "  epoch                   =                   1.02\n",
      "  eval_accuracy           =                 0.6741\n",
      "  eval_loss               =                99.9082\n",
      "  eval_mse                =              9646.5236\n",
      "  eval_runtime            =             0:00:04.93\n",
      "  eval_samples_per_second =                202.537\n",
      "  eval_steps_per_second   =                  12.76\n",
      "  perplexity              = 2.4522313125199866e+43\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.finish()\n",
    "train(model_args_2, data_args, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
